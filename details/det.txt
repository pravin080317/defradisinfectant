Please note that any data refresh, update, or assignment activity should automatically trigger cache invalidation for the affected data scope. For example, when a manager or QC performs an assignment, or when a caseworker updates a sale record or PAD attribute, the related cache entries (for that Billing Authority, batch, or sale) must be cleared in Redis and will repopulate on the next API call. This ensures that users always see the latest information while still benefiting from cached reads for static or unmodified data. The invalidation logic should be handled within the API layer — the database will always remain the authoritative source.


For the SVT screens — particularly the Manager and QC dashboards where data volumes can reach 20–50K records — we need to start treating caching as an essential performance layer rather than an optional enhancement.

From the API side, I’d like the team to explore implementing a two-tier caching strategy: short-term caching at the APIM level (1–2 minutes) for identical GET queries to cut down on repetitive requests, and a more flexible Redis-based cache within the Valuation API (around 5-minute TTL, with key-based invalidation). The API cache should store query slices (e.g., 500-record pages filtered by billing authority, flags, or task status) and support intelligent invalidation when assignments or PAD updates occur. This approach will drastically reduce round-trips to SQL for repeated filter/sort combinations and give us consistent sub-second response times for dashboards.

For the DB team, the expectation is not to introduce caching within the database itself, but to make the data model cache-friendly. That means ensuring all queries are deterministic, using indexed views and composite indexes on the most common filters (Billing Authority, Transaction Date, Sale Price, Task Status, Outlier Flag). The database should remain the single source of truth, but the API caching layer will take on the load for high-frequency reads. We need you to review query stability and confirm that paged queries (OFFSET/FETCH or keyset) perform consistently under 50K rows so they can safely be cached and reused by the API layer.
